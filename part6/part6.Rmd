---
title: ''
author: "you"
date: "5/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Learning Objectives

- **Learn** about t Tests and how to **apply** them to your dataset



## The Dataset
A study by Goran et.al (1996) examined the accuracy of some widely used body-composition techniques for children using three different methods: dual-energy X-ray absorptiometry (`DXA`) technique, skin-fold thickness (`ST`), and bioelectric resistance (`BR`). Subjects were children between 4 and 10 years old. Data were collected on 98 subjects (49 males and 49 females). One purpose of the study was to determine whether there was a difference in fat mass measurements using `DXA` (considered the gold standard method) compared to the skin-fold thickness method. We also wish to determine if `DXA` levels are significantly different between males and females.



## Getting set up

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(readxl)
library(janitor)
```


```{r message=FALSE}
body_comp <- read_csv('data/body_composition.csv', na="NA") %>%
  clean_names()
```

## Exploratory Data Analysis

Before we do any statistical tests on our data, we should first visualize it. Since our ultimate goal is to examine the differences between bodyfat measurement methods, let's create boxplots that illustrate this difference, if any. Notice that the `aes()` for `ggplot()` only accepts one `x` value and one `y` value, but we have three columns we'd like to compare (`dxa`, `st`, `br`). So, we  need to convert our data to long format using `pivot_longer()`.

```{r}
body_comp_long <- body_comp %>%
  pivot_longer(cols = c('dxa', 'st', 'br'),
               names_to = 'method',
               values_to = 'body_fat_percentage')
```

Now that we've done that, we can set `x = method` and `y = body_fat_percentage`.

```{r warning=FALSE}
ggplot(body_comp_long) +
  aes(x = method, y = body_fat_percentage, fill = method) +
  geom_boxplot()
```

It appears that our measurements are close to one another, but there are some noticable differences.


## t-Test

Briefly, a t-Test should be used when examining whether the mean **between two groups** are similar This means that the measurements must be **numeric** (there are other tests for categorical data). The null hypothesis for a t-test is that the two means are equal, and the alternative is that they are not.


> One purpose of the study was to determine whether there was a difference in fat mass measurements using `dxa` (considered the gold standard method) compared to the skin-fold thickness method (`st`).


Below, we will use a paired t-test. Paired simply means that each group (`dxa` and `st`) each contain measurements for the same subject on corresponding rows. If body fat measurements were collected using `dxa` for children in Group A and `st` for a separate set of children in Group B, then we would *not* use a paired t-test.


### HYPOTHESIS: There is a difference in fat mass measurements between the DXA and skin-fold thickness methods.

```{r}
t.test(body_comp$dxa, body_comp$st, paired=TRUE) %>%
  tidy()
```

We see that `p.value` is equal to `~0.634`; this means **we cannot reject the null hypothesis** (i.e., the difference in body fat measurements between `dxa` and `st` are not statistically different from one another).



## Analysis of Variance (ANOVA)

We've determined that there isn't a statistical difference between `dxa` and `st`, but we also meausured bodyfat using bioelectric resistance, `br`. Maybe we should see if it measures differently from the other two methods. Because a t-test can only be used to measure the differences in means between two groups, we'd have to use multiple t-tests.

But wait, should we do that right away? No, because we'll run into the [Multiple Comparisons Problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)!

Consider what a p-value of 0.05 actually means: if a test is performed at the 0.05 level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. This is an okay risk to take given that we are only performing the t-test once. But if we were to perform the t-test 1,000 times on data where all null hypotheses were true, the expected number of incorrect rejections (also known as false positives or Type I errors) would be 50!

So rather than performing multiple t-tests, we first want to examine whether any of the groups is different from the rest of the groups using an ANOVA (`aov()`).

`aov()` uses the formula interface. The tilde (`~`) can be translated to "is a function of". Below, we are testing whether body fat percentage is a function of the type of body fat measurement method. We pipe the output of `aov()` to `summary()` to get a clearer idea of the output of the ANOVA.

```{r}
aov(body_fat_percentage ~ method, data = body_comp_long) %>%
  summary()
```

The value `Pr(>F)` is what we're interested in. Because it is greater than 0.05, we can conclude that none of the measurement methods is significantly different from the others, and there is no reason to perform multiple t-tests on our dataset.


## Post-hoc Tests

Now *if* our F statistic probability had come back below 0.05, then we could perform multiple post-hoc t-tests. However, we would need to account for false positives by using a correction method (e.g., Bonferroni).

```{r}
pairwise.t.test(body_comp_long$body_fat_percentage, body_comp_long$method, p.adjust = "bonferroni")
```


